# A Gentle Introduction to Fine-tuning for Social Scientists

This is a presentation given on March 8, 2024 that introduces `BERT` and its associateed models to the social sciences community.

In the folder, you will find

1. Slides that briefly go over the concept of Language Model and classical language models in the past years. The slide will also briefly describe the transformer structure and cover how BERT is trained.
2. A tutorial that uses a classification task as an example. Data was downloaded from [Kaggle](https://www.kaggle.com/code/alfarias/huffpost-news-classification-with-distilbert), with some changes to improve the task's usability.

Note: As a gentle and new-user friendly introduction, we mostly used `Transformer`'s autoclass model and predefined class of trainer. As a result, minimal prior exposure to `PyTorch` is required. However, to work with neural networks, I would recommend learning `PyTorch` or related frameworks.
